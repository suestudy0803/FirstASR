# # app.py
# import numpy as np
# import gradio as gr
# import torch
# import librosa
# from espnet_model_zoo.downloader import ModelDownloader
# from espnet2.bin.asr_inference import Speech2Text
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# import re

# # ====== ì„¤ì • ======
# TARGET_SR = 16000
# TAG = "Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best"

# # ë§ë²„í¼ ê¸¸ì´(ì´ˆ)ì™€ ìµœì†Œ ë””ì½”ë”© ê¸¸ì´
# RING_SECONDS = 8          # ìµœê·¼ 8ì´ˆë§Œ ìœ ì§€
# MIN_DECODE_SECONDS = 1.0  # ìµœì†Œ 1ì´ˆ ì´ìƒì¼ ë•Œë§Œ ë””ì½”ë”©

# RING_SAMPLES = TARGET_SR * RING_SECONDS
# MIN_DECODE_SAMPLES = int(TARGET_SR * MIN_DECODE_SECONDS)

# # ìŠ¤ë§ˆíŠ¸ ë³‘í•© ì„¤ì •
# COSINE_THRESHOLD = 0.3    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’

# # ====== ëª¨ë¸ ë¡œë“œ ======
# print(">>> Loading ASR model (first run may take a while)...")
# d = ModelDownloader()
# device = "cuda" if torch.cuda.is_available() else "cpu"
# speech2text = Speech2Text(
#     **d.download_and_unpack(TAG),
#     device=device,
#     # í•„ìš” ì‹œ ì¡°ì •:
#     # ctc_weight=0.7,
#     # beam_size=5,
#     # nbest=1,
#     # minlenratio=0.0,
#     # maxlenratio=0.0,
# )
# print(f">>> Model ready on: {device}")

# # ====== ì „ì—­ ë²„í¼: ìµœê·¼ ì˜¤ë””ì˜¤ë§Œ ìœ ì§€ ======
# wav_buf = np.array([], dtype=np.float32)

# def _resample_to_16k(y: np.ndarray, sr: int) -> np.ndarray:
#     """ì›ë³¸ SR ìœ ì§€í•´ì„œ ì½ê³ , ì—¬ê¸°ì„œë§Œ 16kë¡œ 1íšŒ ë¦¬ìƒ˜í”Œ"""
#     if sr == TARGET_SR:
#         return y.astype(np.float32, copy=False)
#     y16 = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR, res_type="polyphase")
#     return y16.astype(np.float32, copy=False)

# def _append_ring(buf: np.ndarray, chunk: np.ndarray) -> np.ndarray:
#     """buf ë’¤ì— chunkë¥¼ ë¶™ì´ê³ , ìµœê·¼ RING_SECONDSë§Œ ìœ ì§€"""
#     if buf.size == 0:
#         cat = chunk
#     else:
#         cat = np.concatenate([buf, chunk])
#     if cat.shape[0] > RING_SAMPLES:
#         cat = cat[-RING_SAMPLES:]  # ë’¤ì—ì„œ RING_SECONDSë§Œ ìœ ì§€
#     return cat

# def do_cosimilarity(text1, text2):
#     """ë‘ í…ìŠ¤íŠ¸ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
#     if not text1.strip() or not text2.strip():
#         return 0.0
    
#     try:
#         # TF-IDF ë²¡í„°í™”
#         vectorizer = TfidfVectorizer().fit([text1, text2])
#         tfidf = vectorizer.transform([text1, text2])
#         # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
#         cosine_sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
#         return cosine_sim
#     except:
#         return 0.0

# def find_overlap_and_merge(text1, text2):
#     """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì„œ ë³‘í•©"""
#     if not text1.strip():
#         return text2
#     if not text2.strip():
#         return text1
    
#     # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
#     words1 = text1.strip().split()
#     words2 = text2.strip().split()
    
#     max_overlap = 0
#     best_overlap_pos = -1
    
#     # text1ì˜ ë’·ë¶€ë¶„ê³¼ text2ì˜ ì•ë¶€ë¶„ì—ì„œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
#     min_len = min(len(words1), len(words2))
#     for i in range(1, min_len + 1):
#         if words1[-i:] == words2[:i]:
#             if i > max_overlap:
#                 max_overlap = i
#                 best_overlap_pos = i
    
#     if best_overlap_pos > 0:
#         # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë³‘í•©
#         merged = words1 + words2[best_overlap_pos:]
#         return ' '.join(merged)
#     else:
#         # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ì—°ê²°
#         return text1 + ' ' + text2

# def smart_concat(text1, text2):
#     """ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì—°ê²°: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ overlap ì—¬ë¶€ ê²°ì •"""
#     if not text1.strip():
#         return text2
#     if not text2.strip():
#         return text1
    
#     # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
#     similarity = do_cosimilarity(text1, text2)
#     print(f"[smart_concat] Cosine similarity: {similarity:.3f}")
    
#     if similarity > COSINE_THRESHOLD:
#         # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ overlapì„ ì°¾ì•„ì„œ ë³‘í•©
#         merged = find_overlap_and_merge(text1, text2)
#         print(f"[smart_concat] Overlap merge applied")
#         return merged
#     else:
#         # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ë‹¨ìˆœ ì—°ê²°
#         merged = text1 + ' ' + text2
#         print(f"[smart_concat] Simple concatenation applied")
#         return merged

# def transcribe_chunk(file_path: str | None, running_text: str):
#     """Gradio ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì½œë°± (ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ)"""
#     global wav_buf

#     if running_text is None:
#         running_text = ""
#     if not file_path:
#         return running_text, running_text

#     # 1) íŒŒì¼â†’íŒŒí˜• (ì›ë³¸ SR ìœ ì§€)
#     y, sr = librosa.load(file_path, sr=None, mono=True)
#     # 2) 16kë¡œ í•œ ë²ˆë§Œ ë¦¬ìƒ˜í”Œ
#     y = _resample_to_16k(y, sr)

#     # 3) ë§ë²„í¼ì— ëˆ„ì 
#     wav_buf = _append_ring(wav_buf, y)
#     cur_sec = wav_buf.shape[0] / TARGET_SR
#     print(f"[buf] {cur_sec:.2f}s @16k, dtype={wav_buf.dtype}")

#     # 4) ë„ˆë¬´ ì§§ìœ¼ë©´ ë””ì½”ë”© ìƒëµ(ë¶ˆí•„ìš” í˜¸ì¶œ/ì¡ìŒ ì¶œë ¥ ë°©ì§€)
#     if wav_buf.shape[0] < MIN_DECODE_SAMPLES:
#         return running_text, running_text

#     # 5) ìµœê·¼ êµ¬ê°„ë§Œ ë””ì½”ë”©
#     nbests = speech2text(wav_buf)
#     hyp = nbests[0][0] if nbests and nbests[0] else ""
    
#     if not hyp.strip():
#         return running_text, running_text

#     # 6) ìŠ¤ë§ˆíŠ¸ ë³‘í•© ì ìš©
#     merged = smart_concat(running_text, hyp)
    
#     print(f"[transcribe] Previous: '{running_text}'")
#     print(f"[transcribe] New hyp: '{hyp}'")
#     print(f"[transcribe] Merged: '{merged}'")
    
#     return merged, merged

# def clear_state():
#     """UIì™€ ë²„í¼ ë™ì‹œ ì´ˆê¸°í™”"""
#     global wav_buf
#     wav_buf = np.array([], dtype=np.float32)
#     return "", ""

# with gr.Blocks(title="Real-time ASR with Smart Concatenation") as demo:
#     gr.Markdown("## ğŸ¤ Real-time ASR with Smart Text Merging\në§ˆì´í¬ë¥¼ ì¼œë©´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")

#     state = gr.State("")
#     mic = gr.Audio(
#         sources=["microphone"],
#         streaming=True,
#         type="filepath",  # íŒŒì¼ ê²½ë¡œë¡œ ì½œë°±ì— ì „ë‹¬
#         format="wav",
#         label="Microphone (streaming)",
#         every = 3,
#     )
#     out = gr.Textbox(label="Transcript (smart concatenated)", lines=8, interactive=False)

#     # ì£¼ê¸° ì„¤ì •ì€ stream()ì—!
#     mic.stream(fn=transcribe_chunk, inputs=[mic, state], outputs=[state, out])

#     gr.Button("Clear").click(fn=clear_state, inputs=None, outputs=[state, out])

# if __name__ == "__main__":
#     # ì „ì—­ ë²„í¼ ì¶©ëŒì„ í”¼í•˜ë ¤ë©´ concurrency_count=1 ê¶Œì¥
#     demo.queue().launch()  # ê³µìœ  í•„ìš” ì‹œ launch(share=True)


# app.py
import numpy as np
import gradio as gr
import torch
import librosa
from espnet_model_zoo.downloader import ModelDownloader
from espnet2.bin.asr_inference import Speech2Text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import jellyfish

# ====== ì„¤ì • ======
TARGET_SR = 16000
TAG = "Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best"

# ë§ë²„í¼ ê¸¸ì´(ì´ˆ)ì™€ ìµœì†Œ ë””ì½”ë”© ê¸¸ì´
RING_SECONDS = 8          # ìµœê·¼ 8ì´ˆë§Œ ìœ ì§€
MIN_DECODE_SECONDS = 1.0  # ìµœì†Œ 1ì´ˆ ì´ìƒì¼ ë•Œë§Œ ë””ì½”ë”©

RING_SAMPLES = TARGET_SR * RING_SECONDS
MIN_DECODE_SAMPLES = int(TARGET_SR * MIN_DECODE_SECONDS)

# ìŠ¤ë§ˆíŠ¸ ë³‘í•© ì„¤ì •
#COSINE_THRESHOLD = 0.3    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’

# ====== ëª¨ë¸ ë¡œë“œ ======
print(">>> Loading ASR model (first run may take a while)...")
d = ModelDownloader()
device = "cuda" if torch.cuda.is_available() else "cpu"
speech2text = Speech2Text(
    **d.download_and_unpack(TAG),
    device=device,
    # í•„ìš” ì‹œ ì¡°ì •:
    # ctc_weight=0.7,
    # beam_size=5,
    # nbest=1,
    # minlenratio=0.0,
    # maxlenratio=0.0,
)
print(f">>> Model ready on: {device}")

# ====== ì „ì—­ ë²„í¼: ìµœê·¼ ì˜¤ë””ì˜¤ë§Œ ìœ ì§€ ======
wav_buf = np.array([], dtype=np.float32)

def _resample_to_16k(y: np.ndarray, sr: int) -> np.ndarray:
    """ì›ë³¸ SR ìœ ì§€í•´ì„œ ì½ê³ , ì—¬ê¸°ì„œë§Œ 16kë¡œ 1íšŒ ë¦¬ìƒ˜í”Œ"""
    if sr == TARGET_SR:
        return y.astype(np.float32, copy=False)
    y16 = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR, res_type="polyphase")
    return y16.astype(np.float32, copy=False)

def _append_ring(buf: np.ndarray, chunk: np.ndarray) -> np.ndarray:
    """buf ë’¤ì— chunkë¥¼ ë¶™ì´ê³ , ìµœê·¼ RING_SECONDSë§Œ ìœ ì§€"""
    if buf.size == 0:
        cat = chunk
    else:
        cat = np.concatenate([buf, chunk])
    if cat.shape[0] > RING_SAMPLES:
        cat = cat[-RING_SAMPLES:]  # ë’¤ì—ì„œ RING_SECONDSë§Œ ìœ ì§€
    return cat

# def do_cosimilarity(text1, text2):
#     """ë‘ í…ìŠ¤íŠ¸ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
#     if not text1.strip() or not text2.strip():
#         return 0.0
    
#     try:
#         # TF-IDF ë²¡í„°í™”
#         vectorizer = TfidfVectorizer().fit([text1, text2])
#         tfidf = vectorizer.transform([text1, text2])
#         # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
#         cosine_sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
#         return cosine_sim
#     except:
#         return 0.0

def phonetic_similarity(text1: str, text2: str) -> bool:
    """
    ë¬¸ì¥ ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ Soundex/Metaphone ë³€í™˜ í›„ ê°™ìœ¼ë©´ ìœ ì‚¬í•˜ë‹¤ê³  ê°„ì£¼.
    ì°¸ê³ : jellyfishëŠ” ë‹¨ì–´ ë‹¨ìœ„ì—ì„œ ë” ì‹ ë¢°ë„ê°€ ë†’ìŒ. í•„ìš”í•˜ë©´
    ë§ˆì§€ë§‰ k-ë‹¨ì–´ vs ì²« k-ë‹¨ì–´ ë¹„êµë¡œ í™•ì¥ ê°€ëŠ¥.
    """
    t1 = text1.strip()
    t2 = text2.strip()
    if not t1 or not t2:
        return False

    try:
        sdx1 = jellyfish.soundex(t1)
        sdx2 = jellyfish.soundex(t2)
    except Exception:
        sdx1 = sdx2 = ""

    try:
        meta1 = jellyfish.metaphone(t1)
        meta2 = jellyfish.metaphone(t2)
    except Exception:
        meta1 = meta2 = ""

    return (sdx1 == sdx2) or (meta1 == meta2)


def find_overlap_and_merge(text1, text2):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì„œ ë³‘í•©"""
    if not text1.strip():
        return text2
    if not text2.strip():
        return text1
    
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    words1 = text1.strip().split()
    words2 = text2.strip().split()
    
    max_overlap = 0
    best_overlap_pos = -1
    
    # text1ì˜ ë’·ë¶€ë¶„ê³¼ text2ì˜ ì•ë¶€ë¶„ì—ì„œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
    min_len = min(len(words1), len(words2))
    for i in range(1, min_len + 1):
        if words1[-i:] == words2[:i]:
            if i > max_overlap:
                max_overlap = i
                best_overlap_pos = i
    
    if best_overlap_pos > 0:
        # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë³‘í•©
        merged = words1 + words2[best_overlap_pos:]
        return ' '.join(merged)
    else:
        # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ì—°ê²°
        return text1 + ' ' + text2

# def smart_concat(text1, text2):
#     """ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì—°ê²°: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ overlap ì—¬ë¶€ ê²°ì •"""
#     if not text1.strip():
#         return text2
#     if not text2.strip():
#         return text1
    
#     # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
#     similarity = do_cosimilarity(text1, text2)
#     print(f"[smart_concat] Cosine similarity: {similarity:.3f}")
    
#     if similarity > COSINE_THRESHOLD:
#         # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ overlapì„ ì°¾ì•„ì„œ ë³‘í•©
#         merged = find_overlap_and_merge(text1, text2)
#         print(f"[smart_concat] Overlap merge applied")
#         return merged
#     else:
#         # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ë‹¨ìˆœ ì—°ê²°
#         merged = text1 + ' ' + text2
#         print(f"[smart_concat] Simple concatenation applied")
#         return merged

def smart_concat(text1, text2):
    """ë°œìŒ ê¸°ë°˜ ìœ ì‚¬ë„(jellyfish)ë¡œ overlap ë³‘í•© ì—¬ë¶€ ê²°ì •"""
    if not text1.strip():
        return text2
    if not text2.strip():
        return text1

    similar = phonetic_similarity(text1, text2)
    print(f"[smart_concat] Phonetic similar?: {bool(similar)}")

    if similar:
        merged = find_overlap_and_merge(text1, text2)
        print(f"[smart_concat] Phonetic-overlap merge applied")
        return merged
    else:
        merged = text1 + ' ' + text2
        print(f"[smart_concat] Simple concatenation applied")
        return merged

def transcribe_chunk(file_path: str | None, running_text: str):
    """Gradio ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì½œë°± (ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ)"""
    global wav_buf

    if running_text is None:
        running_text = ""
    if not file_path:
        return running_text, running_text

    # 1) íŒŒì¼â†’íŒŒí˜• (ì›ë³¸ SR ìœ ì§€)
    y, sr = librosa.load(file_path, sr=None, mono=True)
    # 2) 16kë¡œ í•œ ë²ˆë§Œ ë¦¬ìƒ˜í”Œ
    y = _resample_to_16k(y, sr)

    # 3) ë§ë²„í¼ì— ëˆ„ì 
    wav_buf = _append_ring(wav_buf, y)
    cur_sec = wav_buf.shape[0] / TARGET_SR
    print(f"[buf] {cur_sec:.2f}s @16k, dtype={wav_buf.dtype}")

    # 4) ë„ˆë¬´ ì§§ìœ¼ë©´ ë””ì½”ë”© ìƒëµ(ë¶ˆí•„ìš” í˜¸ì¶œ/ì¡ìŒ ì¶œë ¥ ë°©ì§€)
    if wav_buf.shape[0] < MIN_DECODE_SAMPLES:
        return running_text, running_text

    # 5) ìµœê·¼ êµ¬ê°„ë§Œ ë””ì½”ë”©
    nbests = speech2text(wav_buf)
    hyp = nbests[0][0] if nbests and nbests[0] else ""
    
    if not hyp.strip():
        return running_text, running_text

    # 6) ìŠ¤ë§ˆíŠ¸ ë³‘í•© ì ìš©
    merged = smart_concat(running_text, hyp)
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ë’·ë¶€ë¶„ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    if len(merged.split()) > 100:
        words = merged.split()
        merged = ' '.join(words[-80:])  # ë§ˆì§€ë§‰ 80ë‹¨ì–´ë§Œ ìœ ì§€
        print(f"[transcribe] Text truncated to last 80 words")
    
    print(f"[transcribe] Previous: '{running_text[:50]}{'...' if len(running_text) > 50 else ''}'")
    print(f"[transcribe] New hyp: '{hyp}'")
    print(f"[transcribe] Merged: '{merged[:50]}{'...' if len(merged) > 50 else ''}'")
    
    return merged, merged

def clear_state():
    """UIì™€ ë²„í¼ ë™ì‹œ ì´ˆê¸°í™”"""
    global wav_buf
    wav_buf = np.array([], dtype=np.float32)
    return "", ""

with gr.Blocks(title="Real-time ASR with Smart Concatenation") as demo:
    gr.Markdown("## ğŸ¤ Real-time ASR with Smart Text Merging\në§ˆì´í¬ë¥¼ ì¼œë©´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")

    state = gr.State("")
    mic = gr.Audio(
        sources=["microphone"],
        streaming=True,
        type="filepath",  # íŒŒì¼ ê²½ë¡œë¡œ ì½œë°±ì— ì „ë‹¬
        format="wav",
        label="Microphone (streaming)",
        every = 3,
    )
    out = gr.Textbox(label="Transcript (smart concatenated)", lines=8, interactive=False)

    # ì£¼ê¸° ì„¤ì •ì€ stream()ì—!
    mic.stream(fn=transcribe_chunk, inputs=[mic, state], outputs=[state, out])

    gr.Button("Clear").click(fn=clear_state, inputs=None, outputs=[state, out])

if __name__ == "__main__":
    # ì „ì—­ ë²„í¼ ì¶©ëŒì„ í”¼í•˜ë ¤ë©´ concurrency_count=1 ê¶Œì¥
    demo.queue().launch()  # ê³µìœ  í•„ìš” ì‹œ launch(share=True)

