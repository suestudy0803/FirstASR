import numpy as np
import gradio as gr
import torch
import librosa
from espnet_model_zoo.downloader import ModelDownloader
from espnet2.bin.asr_inference import Speech2Text

# ====== ì„¤ì • ======
TARGET_SR = 16000
TAG = "Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best"

# ë§ë²„í¼ ê¸¸ì´(ì´ˆ)ì™€ ìµœì†Œ ë””ì½”ë”© ê¸¸ì´
RING_SECONDS = 4      # ìµœê·¼ 8ì´ˆë§Œ ìœ ì§€
MIN_DECODE_SECONDS = 1.0  # ìµœì†Œ 1ì´ˆ ì´ìƒì¼ ë•Œë§Œ ë””ì½”ë”©

RING_SAMPLES = TARGET_SR * RING_SECONDS
MIN_DECODE_SAMPLES = int(TARGET_SR * MIN_DECODE_SECONDS)

# ====== ëª¨ë¸ ë¡œë“œ ======
print(">>> Loading ASR model (first run may take a while)...")
d = ModelDownloader()
device = "cuda" if torch.cuda.is_available() else "cpu"
speech2text = Speech2Text(
    **d.download_and_unpack(TAG),
    device=device,
    # í•„ìš” ì‹œ ì¡°ì •:s
    # ctc_weight=0.7,
    # beam_size=5,
    # nbest=1,
    # minlenratio=0.0,
    # maxlenratio=0.0,
)
print(f">>> Model ready on: {device}")

# ====== ì „ì—­ ë²„í¼: ìµœê·¼ ì˜¤ë””ì˜¤ë§Œ ìœ ì§€ ======
wav_buf = np.array([], dtype=np.float32)

# ---------------------------
# ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ & ë§ë²„í¼ ìœ í‹¸
# ---------------------------
def _resample_to_16k(y: np.ndarray, sr: int) -> np.ndarray:
    """ì›ë³¸ SR ìœ ì§€í•´ì„œ ì½ê³ , ì—¬ê¸°ì„œë§Œ 16kë¡œ 1íšŒ ë¦¬ìƒ˜í”Œ"""
    if sr == TARGET_SR:
        return y.astype(np.float32, copy=False)
    y16 = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR, res_type="polyphase")
    return y16.astype(np.float32, copy=False)

def _append_ring(buf: np.ndarray, chunk: np.ndarray) -> np.ndarray:
    """buf ë’¤ì— chunkë¥¼ ë¶™ì´ê³ , ìµœê·¼ RING_SECONDSë§Œ ìœ ì§€"""
    if buf.size == 0:
        cat = chunk
    else:
        cat = np.concatenate([buf, chunk])
    if cat.shape[0] > RING_SAMPLES:
        cat = cat[-RING_SAMPLES:]  # ë’¤ì—ì„œ RING_SECONDSë§Œ ìœ ì§€
    return cat

# ---------------------------
# ì†Œë¦¬(íŒŒí˜•) ê²¹ì¹¨ ê°ì§€/ì œê±° (NCC ê³„ì—´)
# ---------------------------
def _zscore(x: np.ndarray) -> np.ndarray:
    if x.size == 0:
        return x
    m = np.mean(x)
    s = np.std(x)
    if s < 1e-8:
        return x * 0.0
    return (x - m) / s

def _cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    # ë‘ ë²¡í„°ê°€ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì•ìª½ì„ ìë¦„
    L = min(a.size, b.size)
    if L == 0:
        return 0.0
    a = a[-L:]
    b = b[:L]
    an = _zscore(a)
    bn = _zscore(b)
    denom = np.linalg.norm(an) * np.linalg.norm(bn)
    if denom < 1e-8:
        return 0.0
    return float(np.dot(an, bn) / denom)

def trim_audio_overlap(prev_buf: np.ndarray,
                       new_chunk: np.ndarray,
                       sr: int,
                       max_overlap_sec: float = 1.0,
                       min_overlap_sec: float = 0.08,
                       hop_ms: int = 10,
                       sim_threshold: float = 0.75):
    """
    prev_bufì˜ 'ë§ˆì§€ë§‰ ìµœëŒ€ max_overlap_sec'ê³¼ new_chunkì˜ 'ì²˜ìŒ'ì„ ê²¹ì¹¨ìœ¼ë¡œ ê°€ì •í•˜ê³ 
    Lì„ í¬ê²Œâ†’ì‘ê²Œë¡œ ìŠ¤ìº”í•˜ë©° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ sim_threshold ì´ìƒì¸ ê°€ì¥ ê¸´ Lì„ ì°¾ìŒ.
    ì°¾ìœ¼ë©´ new_chunk[:L]ë¥¼ ì œê±°í•˜ì—¬ ë°˜í™˜.
    ë°˜í™˜: (trimmed_chunk, detected_overlap_samples, best_sim)
    """
    if prev_buf.size == 0 or new_chunk.size == 0:
        return new_chunk, 0, 0.0

    max_L = int(max_overlap_sec * sr)
    min_L = int(min_overlap_sec * sr)
    hop = int(hop_ms * sr / 1000)

    tail = prev_buf[-max_L:] if prev_buf.size >= max_L else prev_buf.copy()
    head = new_chunk[:max_L] if new_chunk.size >= max_L else new_chunk.copy()

    # ìŠ¤ìº” ì‹œì‘ ê¸¸ì´: tail/headì˜ ê³µí†µ ê°€ëŠ¥í•œ ìµœëŒ€ ê¸¸ì´
    start_L = min(tail.size, head.size)
    if start_L < min_L:
        return new_chunk, 0, 0.0

    best_L = 0
    best_sim = -1.0

    # Lì„ í¬ê²Œ->ì‘ê²Œë¡œ ë‚´ë ¤ê°€ë©° ìœ ì‚¬ë„ ê²€ì‚¬ (ì—°ì‚°ëŸ‰ ì¤„ì´ë ¤ hop ê°„ê²© ì‚¬ìš©)
    for L in range(start_L, min_L - 1, -hop):
        sim = _cos_sim(tail[-L:], head[:L])
        if sim > best_sim:
            best_sim = sim
            best_L = L
        if sim >= sim_threshold:
            # ì²«ë²ˆì§¸ë¡œ ì„ê³„ê°’ì„ ë„˜ëŠ” ê°€ì¥ ê¸´ Lì„ ì±„íƒí•˜ê³  ì¤‘ë‹¨
            break

    if best_sim >= sim_threshold and best_L >= min_L:
        # ê²¹ì¹¨ êµ¬ê°„ ì œê±°
        trimmed = new_chunk[best_L:]
        print(f"[overlap] detected {best_L/sr:.3f}s (sim={best_sim:.3f}) -> trimming")
        return trimmed, best_L, best_sim
    else:
        return new_chunk, 0, float(best_sim)

# ---------------------------
# ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
# ---------------------------
def transcribe_chunk(file_path: str | None, running_text: str):
    """Gradio ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì½œë°± (ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ)"""
    global wav_buf

    if running_text is None:
        running_text = ""
    if not file_path:
        return running_text, running_text

    # 1) íŒŒì¼â†’íŒŒí˜• (ì›ë³¸ SR ìœ ì§€)
    y, sr = librosa.load(file_path, sr=None, mono=True)
    # 2) 16kë¡œ í•œ ë²ˆë§Œ ë¦¬ìƒ˜í”Œ
    y = _resample_to_16k(y, sr)

    # 2-1) 'ì†Œë¦¬ ì°¨ì›' ê²¹ì¹¨ ì œê±° (prev tail vs new head)
    y, ovl_samps, ovl_sim = trim_audio_overlap(
        prev_buf=wav_buf,
        new_chunk=y,
        sr=TARGET_SR,
        max_overlap_sec=1.0,   # í•„ìš”ì— ë”°ë¼ 0.5~1.5s ì¡°ì •
        min_overlap_sec=0.08,  # 80ms ì´ìƒì¼ ë•Œë§Œ ê²¹ì¹¨ìœ¼ë¡œ ì¸ì •
        hop_ms=10,             # 10ms ìŠ¤í…
        sim_threshold=0.75     # 0.7~0.85 ì‚¬ì´ ì¡°ì ˆ
    )

    # 3) ë§ë²„í¼ì— ëˆ„ì 
    wav_buf = _append_ring(wav_buf, y)
    cur_sec = wav_buf.shape[0] / TARGET_SR
    print(f"[buf] {cur_sec:.2f}s @16k, dtype={wav_buf.dtype}")

    # 4) ë„ˆë¬´ ì§§ìœ¼ë©´ ë””ì½”ë”© ìƒëµ(ë¶ˆí•„ìš” í˜¸ì¶œ/ì¡ìŒ ì¶œë ¥ ë°©ì§€)
    if wav_buf.shape[0] < MIN_DECODE_SAMPLES:
        return running_text, running_text

    # 5) ìµœê·¼ êµ¬ê°„ë§Œ ë””ì½”ë”©
    nbests = speech2text(wav_buf)
    hyp = nbests[0][0] if nbests and nbests[0] else ""

    if not hyp.strip():
        return running_text, running_text

    # 6) í…ìŠ¤íŠ¸ëŠ” ë‹¨ìˆœíˆ ì´ì–´ë¶™ì„ (ì˜¤ë””ì˜¤ì—ì„œ ì´ë¯¸ ê²¹ì¹¨ ì œê±°ë¨)
    merged = (running_text + " " + hyp).strip() if running_text.strip() else hyp

    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ë’·ë¶€ë¶„ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    if len(merged.split()) > 100:
        words = merged.split()
        merged = ' '.join(words[-80:])
        print(f"[transcribe] Text truncated to last 80 words")

    print(f"[transcribe] New hyp: '{hyp}'")
    print(f"[transcribe] Merged: '{merged[:50]}{'...' if len(merged) > 50 else ''}'")

    return merged, merged

def clear_state():
    """UIì™€ ë²„í¼ ë™ì‹œ ì´ˆê¸°í™”"""
    global wav_buf
    wav_buf = np.array([], dtype=np.float32)
    return "", ""

# ---------------------------
# UI
# ---------------------------
with gr.Blocks(title="Real-time ASR (Audio-level Overlap Trimming)") as demo:
    gr.Markdown("## ğŸ¤ Real-time ASR (Audio-level Overlap Trimming)\nì˜¤ë””ì˜¤ íŒŒí˜•ì—ì„œ ê²¹ì¹¨ì„ ê°ì§€/ì œê±°í•œ ë’¤ ì¸ì‹í•©ë‹ˆë‹¤.")

    state = gr.State("")
    mic = gr.Audio(
        sources=["microphone"],
        streaming=True,
        type="filepath",  # íŒŒì¼ ê²½ë¡œë¡œ ì½œë°±ì— ì „ë‹¬
        format="wav",
        label="Microphone (streaming)",
        every=2,          # ì½œë°± ì£¼ê¸°(ì´ˆ) - í•„ìš” ì‹œ ì¡°ì •
    )
    out = gr.Textbox(label="Transcript", lines=8, interactive=False)

    mic.stream(fn=transcribe_chunk, inputs=[mic, state], outputs=[state, out])
    gr.Button("Clear").click(fn=clear_state, inputs=None, outputs=[state, out])

if __name__ == "__main__":
    # ì „ì—­ ë²„í¼ ì¶©ëŒì„ í”¼í•˜ë ¤ë©´ concurrency_count=1 ê¶Œì¥
    demo.queue().launch()
